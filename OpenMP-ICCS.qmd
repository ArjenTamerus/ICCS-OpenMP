---
title: "Intro to OpenMP for GPUs"
subtitle: "2024 ICCS Summer School"
author: "Arjen Tamerus and Chris Edsall"
format: beamer
---

# OpenMP for ICCS

## Goals

 - Get you going with OpenMP offloading

 - ~ 1 hour introduction to OpenMP
 - ~ 2 hours practical exercises

## Restrictions

 - OpenMP is a big specification
 - We'll focus on a useful subset
	 - Aim is to get you familiar with OpenMP

## Parallelism in compute

 - Most common programming languages are (still) inherently serial\*:
   - C, Fortran, Python (mostly)

 - If we want to scale up our calculations, we usually use any number of
   frameworks or tools to 
 - Serial limitations
 - Types of parallelism:
  - Scale-out/Distributed (MPI, not in scope)
  - Intra-node: threading (OpenMP-CPU)
  - Accelerators (e.g. GPU, A100/PVC)
    - main focus of talk

 - Amdahl's Law?

## OpenMP
 - Directive-based: annotations in your source code
 - Only for: C, C++, Fortran

 - Origins: Fortran, then C, then unified
  - CPU-only until 4.5

 - GPU functionality developed from 5.x onwards; v6 releasing soon
 - Feature parity between languages

 - Today's focus is GPU, but will cover CPU briefly

## OpenMP - Modi Operandi
 - Mention non-loop parallelism at the end, not focus of the session
  - tasking etc
  (mention all CPU methods work for GPU as well, tasks + events for GPU kernel
  dependencies & scheduling - perhaps at end, quite 

## OpenMP - CPU

```fortran
 - !$omp parallel do
   - !$omp parallel + !$omp do
```

 - Thread parameters: identification, configuration etc

 - default(none) shared(xyz) private(abc) firstprivate(def) ..

 - Mention NUMA effects?
   - to the extent of: _just throw more threads at it_ may not always be the
     best solution, NUMA effects + memory bandwidth + cache thrashing + locking
       - Oh look it got complicated/

## OpenMP - GPU

 - !$omp parallel do -> !$omp target parallel do
  - works!
  - Terribly slow!

 - > GPU architecture intermezzo

## GPU Architecture
 - GPUs have a fundamentally different architecture to CPUs
 - My favourite bad analogy: trucks on a motorway vs freight train/container
   ship
 - <image for analogy>

## GPU Architecture
 - <GPU architecture image (vs cpu?) >

 - Hierarchical nature
 - OpenMP maps quite closely

## OpenMP - GPU 2
 - expand into target teams distribute parallel do simd

## OpenMP - GPU memory
 - Offload device & CPU (usually) have separate physical memory
 [RAM]      [VRAM]
   |          |
   |          |
 [CPU]------[GPU]



## OpenMP - data movement
 - By default, the OpenMP runtime will copy _all_ required data to and from the
GPU memory
 - Can be extremely inefficient!
 - Can control data movement using the `map` clause
   - Note _map_: the runtime keeps a _mapping_ between host and device memory
   - GPU memory always has a host equivalent!*

## Data movement - interlude
\pause
 - Moving data between host and device is *slow*
\pause
 - _Avoid_ it if you can!

## The `map` clause
 - Adding a `map` clause to an omp target region provides the compiler with
	 instructions on how to move data around
 - Format:
	   - `map(<operation>:var1[,var2,...])`
		 - Valid operations: `to`, `from`, `tofrom`, `alloc`
		 - `tofrom` is default
	
 (TODO: table-ise ops)

 - Can control data movement per kernel, or create _data regions_
	- Will discuss these in order

## Controlling data movement for a compute kernel
```fortran
!$omp target teams loop
do i=1,N
	c(i) = a(i) + b(i)
end do
!$omp end target teams loop
```
 - By default, _all_ of `a`, `b` and `c` will be copied to the GPU at the start
	 of this kernel, then back to the host after it's finished.
 - Let's look at how the data is actually used!


## Controlling data movement for a compute kernel
```fortran
!$omp target teams loop
do i=1,N
	c(i) = a(i) + b(i)
end do
!$omp end target teams loop
```
 - In this loop we are:
	 - Reading from `a` and `b`
	 - Writing the result to `c`

## Controlling data movement for a compute kernel
```fortran
!$omp target teams loop
do i=1,N
	c(i) = a(i) + b(i)
end do
!$omp end target teams loop
```
 - In this loop we are:
	 - Reading from `a` and `b` -> Their value's don't change!
	 - Writing the result to `c` -> We're not reading, so the GPU doesn't have to
		 know the initial value!


## Controlling data movement for a compute kernel
```fortran
!$omp target teams loop map(to:a(1:N),b(1:N)) map(from:c(1:N))
do i=1,N
	c(i) = a(i) + b(i)
end do
!$omp end target teams loop
```
 - Now we're only copying `a` and `b` to the GPU, and only copying the final
	 value of `c` back to the host.
	  - This halves the amount of data we're moving around!

## Keeping data resident on the GPU
 - We've looked at optimising data movement _per kernel_
 - Often, we'll have multiple kernels operating on the same data
 - How do we avoid moving data around unnecessarily?

## Data regions
 - OpenMP data regions map data to the GPU, and _keep it there_ until the region
	 ends.
 - Can be either _structured_ or _unstructured_

## Data regions - structured
 - We can create structured (or scoped) data regions with the `omp target data`
	 directive
 - Useful when keeping data resident within a specific region of the code (e.g.
	 within a subroutine).

```fortran
!$omp target data map(to:a(1:N)) map(tofrom:b(1:N))

!$omp target teams loop
do i=1,N
	b(i) = a(i)+b(i)
end do
!$omp end target teams

!$omp end target data
```


## Accessing host data within data regions
 - May want to keep data resident on the GPU most of the time, but still need to
	 update on the host.
   - Nested data regions
   - `update` directive

## Update directive
```fortran
!$omp target data map(to:a(1:N)) map(tofrom:b(1:N))

!$omp target teams loop
a
a
a
!$omp end target teams loop

call update_on_host(a)
!$omp target update to(a(1:N))

!$omp target teams loop
a
a
a
!$omp end target teams loop

!$omp end target data
```

## Data regions - unstructured
 - omp target enter/exit data - unstructured/unscoped

## OpenMP - GPU libraries
 - For performance reasons, we might to call out to a GPU-enabled library
	  - e.g. LAPACK, BLAS, FFT, Eigen etc
 - May be vendor-specific (exercise for the reader)
 - Often expect device memory pointers to be passed into them
	
## OpenMP - GPU libraries
 - OpenMP can expose device pointers using the `use_device_addr` directive:

```fortran
integer :: N
real, dimension(:) :: my_array

...

!$omp target data use_device_addr(my_array)
	call my_gpu_library(N, my_array)
!$omp end target data
```
 - also `is_device_ptr` etc

## OpenMP - GPU libraries - target variant
 - The `variant` directive (standardised in OpenMP 5.2) allows developers to
	 specify host and device variants of the same routine, with the same interface
 - OpenMP will choose correct variant based on the type of pointer (host or
	 device) passed to the routine

```fortran
!$omp target data use_device_addr(A,B,C)
!$omp target variant
	call dgemm(..., A, ..., B, ..., C, ...) ! A, B, C are device pointers -> call device routine
!$omp end target variant
!$omp end target data
```

## Exercises
 - On GitHub
 - Using Dawn

## Logging in & getting started
 - Login using:
	 - https://login-web.hpc.cam.ac.uk
	 - ssh <user>@login.hpc.cam.ac.uk

 - `git clone git@github.com:ICCS/openmp.git`
