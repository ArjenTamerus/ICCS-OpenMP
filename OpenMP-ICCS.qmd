---
title: "OpenMP for ICCS"
author: "Arjen Tamerus and Chris Edsall"
format: beamer
---

# OpenMP for ICCS

## Parallelism in compute

 - Most common programming languages are inherently serial\*:
   - C, Fortran, Python (mostly)

 - If we want to scale up our calculations, we usually use any number of
   frameworks or tools to 
 - Serial limitations
 - Types of parallelism:
  - Scale-out/Distributed (MPI, not in scope)
  - Intra-node: threading (OpenMP-CPU)
  - Accelerators (e.g. GPU, A100/PVC)
    - main focus of talk

 - Mention it's quite condensed / only a subset due to limited time

 - Amdahl's Law?

## OpenMP
 - Directive-based: annotations in your source code
 - Only for: C, C++, Fortran

 - Origins: Fortran, then C, then unified
  - CPU-only until 4.5

 - GPU functionality developed from 5.x onwards; v6 releasing soon
 - Feature parity between languages

 - Today's focus is GPU, but will cover CPU briefly

## OpenMP - Modi Operandi
 - Mention non-loop parallelism at the end, not focus of the session
  - tasking etc
  (mention all CPU methods work for GPU as well, tasks + events for GPU kernel
  dependencies & scheduling - perhaps at end, quite 

## OpenMP - CPU

```
 - !$omp parallel do
   - !$omp parallel + !$omp do
```

 - Thread parameters: identification, configuration etc

 - default(none) shared(xyz) private(abc) firstprivate(def) ..

 - Mention NUMA effects?
   - to the extent of: _just throw more threads at it_ may not always be the
     best solution, NUMA effects + memory bandwidth + cache thrashing + locking
       - Oh look it got complicated/

## OpenMP - GPU

 - !$omp parallel do -> !$omp target parallel do
  - works!
  - Terribly slow!

 - > GPU architecture intermezzo

## GPU Architecture
 - GPUs have a fundamentally different architecture to CPUs
 - My favourite bad analogy: trucks on a motorway vs freight train/container
   ship
 - <image for analogy>

## GPU Architecture
 - <GPU architecture image (vs cpu?) >

 - Hierarchical nature
 - OpenMP maps quite closely

## OpenMP - GPU 2
 - expand into target teams distribute parallel do simd

## OpenMP - GPU memory
 - Offload device & CPU (usually) have separate physical memory
 [RAM]      [VRAM]
   |          |
   |          |
 [CPU]------[GPU]



## OpenMP - data movement
 - By default, the OpenMP runtime will copy _all_ required data to and from the
GPU memory
 - Can be extremely inefficient!
 - Can control data movement using the `map` clause
   - Note _map_: the runtime keeps a _mapping_ between host and device memory
   - GPU memory always has a host equivalent!*

## The `map` clause
 - Adding a `map` clause to an omp target region provides the compiler with
	 instructions on how to move data around
 - Format:
	   - `map(<operation>:var1[,var2,...])`
		 - Valid operations: `to`, `from`, `tofrom`, `alloc`
		 - `tofrom` is default
	
 (TODO: table-ise ops)

 - Can control data movement per kernel, or create _data regions_
	- Will discuss these in order

## Controlling data movement for a compute kernel
```
!$omp target teams loop
do i=1,N
	c(i) = a(i) + b(i)
end do
!$omp end target teams loop
```
 - By default, _all_ of `a`, `b` and `c` will be copied to the GPU at the start
	 of this kernel, then back to the host after it's finished.
 - Let's look at how the data is actually used!


## Controlling data movement for a compute kernel
```
!$omp target teams loop
do i=1,N
	c(i) = a(i) + b(i)
end do
!$omp end target teams loop
```
 - In this loop we are:
	 - Reading from `a` and `b`
	 - Writing the result to `c`

## Controlling data movement for a compute kernel
```
!$omp target teams loop
do i=1,N
	c(i) = a(i) + b(i)
end do
!$omp end target teams loop
```
 - In this loop we are:
	 - Reading from `a` and `b` -> Their value's don't change!
	 - Writing the result to `c` -> We're not reading, so the GPU doesn't have to
		 know the initial value!


## Controlling data movement for a compute kernel
```
!$omp target teams loop map(to:a(1:N),b(1:N)) map(from:c(1:N))
do i=1,N
	c(i) = a(i) + b(i)
end do
!$omp end target teams loop
```
 - Now we're only copying `a` and `b` to the GPU, and only copying the final
	 value of `c` back to the host.
	  - This halves the amount of data we're moving around!

## Keeping data resident on the GPU
 - We've looked at optimising data movement _per kernel_
 - Often, we'll have multiple kernels operating on the same data
 - How do we avoid moving data around unnecessarily?

## Data regions
 - OpenMP data regions map data to the GPU, and _keep it there_ until the region
	 ends.
 - Can be either _structured_ or _unstructured_
 - 

## Data regions - structured
 - omp target data - structured/scoped

## Accessing host data within data regions
 - Nested data regions
 - `update` directive

## Data regions - unstructured
 - omp target enter/exit data - unstructured/unscoped

## OpenMP - GPU libraries
 - target variant
 - host\_data use\_device

## Exercises

I guess saxpy?
